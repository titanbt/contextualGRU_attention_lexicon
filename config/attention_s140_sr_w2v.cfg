# tweets are resolved from 1.600.000 tweets
# tweets is divided into 3 files (train, dev, test)
# train and dev have ratio is 6% and 1%
# tweets are preprocessed.
# using semantic rules

[model]
train_file                      = ./data/sen140_sr/train.txt
dev_file                        = ./data/sen140_sr/dev.txt
test_file                       = ./data/sen140_sr/test.txt
word_column                     = 2
label_column                    = 1
oov                             = embedding
fine_tune                       = True
embedding                       = word2vec
embedding_path                  = ./vec/w2v.bin
use_character                   = True
batch_size                      = 100
num_epochs                      = 30
patience                        = 5
num_units                       = 80
num_filters                     = 6,14
filter_size                     = 7,1
peepholes                       = False
grad_clipping                   = 3
dropout                         = 0.5
regular                         = None
gamma                           = 1e-6
learning_rate                   = 0.1
update_algo                     = adadelta
momentum                        = 0.9
decay_rate                      = 0.1
output_predict                  = False
valid_freq                      = 10
L2                              = 0.0001,0.00003,0.000003,0.0001
model_path                      = attention_context
training                        = True
params_file                     = None
lex_path                        = ./resources/lexicons/lex_config.txt
embedding_context               = context
embedding_context_path          = ./vec/deps.contexts.gz